{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-painting",
   "metadata": {},
   "source": [
    "# Homework:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf023e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NAME:**\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:** \n",
    "- The clarity (clean writing) and the organization of the work (numbering the questions appropriately) are taken into account during the correction. \n",
    "- Before uploading your notebook, **Kernel/restart and clear output**. And **verify that your code is running cell after cell.**\n",
    "- This homework is **optional**. It counts at most as 4 **bonus** points on the final grade (over 20) of the course (capped at 20). It must be uploaded back on moodle **BEFORE** the 12th of March at 8PM (Paris time) to be counted, no delay will be accepted. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6f65d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64429d69",
   "metadata": {},
   "source": [
    "Consider the matrices \n",
    "\n",
    "$$A = \\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 2 & 3 & 0 \\\\ 4 & 5 & 6\\end{array}\\right), \\qquad \n",
    "  B = \\left(\\begin{array}{ccc} 1 & 2 & 1 \\\\ 2 & 5 & 4 \\\\ 1 & 4 & 6\\end{array}\\right), $$\n",
    "and the vector $b = \\left(\\begin{array}{c} 1 \\\\ 1 \\\\ 1\\end{array}\\right)$.\n",
    "\n",
    "1) Write down the steps of the forward substitution algorithm to solve the problem $AV = b$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66c108",
   "metadata": {},
   "source": [
    "**Answer**: 1) \n",
    "\n",
    "The equation $AV = b$ translates to solving the following:\n",
    "$$\\begin{pmatrix} v_1 \\\\ 2v_1 + 3v_2 \\\\ 4v_1+5v_2+6v_3 \\end{pmatrix} = \\begin{pmatrix} 1\\\\1\\\\1 \\end{pmatrix}$$\n",
    "Where $v_1,v_2$ and $v_3$ are the coordinates of the solution $V$. Since $A$ is a lower triangular matrix, we can solve for $V$ by solving the linear equations in the order as they come, this leads to:\n",
    "1. $v_1 = 1$\n",
    "2. $v_2 = \\frac{1-2v_1}{3} = -\\frac{1}{3}$\n",
    "3. $v_1 = \\frac{1 - 4v_1 - 5v_2}{6} = -\\frac{2}{9}$\n",
    "\n",
    "Hence, we obtain the unique solution $V = \\begin{pmatrix} 1 \\\\ -\\frac{1}{3} \\\\ -\\frac{2}{9} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 9 \\\\ -3 \\\\ -2 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af57e9b",
   "metadata": {},
   "source": [
    "2) Write down the steps of the Cholesky algorithm to decompose the matrix $B = L L^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea15da5",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "We're looking for a lower triangular matrix $L = \\begin{pmatrix} L_{1,1}&0&0 \\\\ L_{2,1}&L_{2,2}&0 \\\\ L_{3,1}&L_{3,2}&L_{3,3} \\end{pmatrix}$ that satisfies \n",
    "\n",
    "$$B = L L^T = \\begin{pmatrix} L_{1,1}^2 & L_{1,1}L_{2,1}&L_{1,1}L_{3,1} \\\\ L_{1,1}L_{2,1}&L_{2,1}^2 + L_{2,2}^2 & L_{2,1}L_{3,1} + L_{2,2}L_{3,2} \\\\ L_{1,1}L_{3,1}&L_{2,1}L_{3,1} + L_{2,2}L_{3,2}&L_{3,1}^2+L_{3,2}^2+L_{3,3}^2 \\end{pmatrix}$$\n",
    "\n",
    "To solve this system, keeping in mind that both sides of the equation are symmetric, we'll only focus on the lower triangular part of the matrix. We solve the system column-by-column and row-by-row $(1,1) \\rightarrow (2,1) \\rightarrow (3,1) \\rightarrow (2,2) \\rightarrow (3,2) \\rightarrow (3,3)$ and each equation will reveal a new number of the solution $L$. The only times we may have multiple solutions is on the diagonal, where we'll have 2 solutions, being the root of some given number. For this, we'll choose the positive square root of each equation.\n",
    "\n",
    "1) $L_{1,1}^2 = B_{1,1} = 1 \\Longrightarrow L_{1,1} = 1$\n",
    "\n",
    "2) $L_{1,1}L_{2,1} = B_{2,1} = 2 \\Longrightarrow L_{2,1} = 2$\n",
    "\n",
    "3) $L_{1,1}L_{3,1} = B_{3,1} = 1 \\Longrightarrow L_{3,1} = 1$\n",
    "\n",
    "4) $L_{2,1}^2 + L_{2,2}^2 = B_{2,2} = 5 \\Longrightarrow L_{2,2} = 1$\n",
    "\n",
    "5) $L_{2,1}L_{3,1} + L_{2,2}L_{3,2} = B_{3,2} = 4 \\Longrightarrow L_{3,2} = 2$\n",
    "\n",
    "6) $L_{3,1}^2 + L_{3,2}^2 + L_{3,3}^2 = B_{3,3} = 6 \\Longrightarrow L_{3,3} = 1$\n",
    "\n",
    "Hence, we obtain that for $L = \\begin{pmatrix} 1&0&0\\\\2&1&0\\\\1&2&1 \\end{pmatrix}$, we have $B = L L^T$\n",
    "\n",
    "We must mention that in order to perform the Cholesky decomposition on any matrix, we must first check that it is a symmetric positive-definite one. One may verify this by evaluating the characteristic polynomial of &B&, which can be shown to have 3 distinct positive roots, thus impliying the positive-definitivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc42034",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-producer",
   "metadata": {},
   "source": [
    "**Definition:** A matrix $S$ is orthogonal if it is invertible and its inverse satisfies $S^{-1} = S^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-voluntary",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to construct an algorithm providing a decomposition of a matrix $A = S U$ into the product of an orthogonal matrix $S$ and an upper triangular one $U$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-dryer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preliminary computations on orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-booth",
   "metadata": {},
   "source": [
    "1) a) Consider an orthogonal matrix $S$. What values can $det(S)$ have?\n",
    "\n",
    "b) Are all matrices satisfying this property orthogonal? Prove it or give a counterexample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-chemical",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) A known property of transposes gives us the fact that $det(S^T) = det(S)$ for any square matrix $S$. For orthogonal matrices, $Id = S^T S \\Rightarrow 1 = det(S^T) det(S) = det(S)^2$, hence the determinant of $S$ can only be $1$ or $-1$.\n",
    "\n",
    "b) The answer is in the negative, one can take the counter example $A = \\begin{pmatrix} 1&1 \\\\ 0&1 \\end{pmatrix}$. One may check that $det(A) = 1$, but $A^T A = \\begin{pmatrix} 2&1 \\\\ 1&1 \\end{pmatrix}$, so $A^T \\neq A^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-vertical",
   "metadata": {},
   "source": [
    "2) Prove that the product of orthogonal matrices remains an orthogonal matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-accommodation",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Let $A$ and $B$ be two orthogonal matrices. Then we have:\n",
    "$$(AB)^T = B^T A^T = B^{-1} A^{-1} = (AB)^{-1}$$\n",
    "\n",
    "So this proves that $AB$ is orthogonal too, hence the set of orthogonal matrices is stable under matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-spyware",
   "metadata": {},
   "source": [
    "3) Prove that the Euclidean norm of a vector is preserved when multiplying it by an orthogonal matrix $S$, i.e. \n",
    "\n",
    "$$\\|S V\\| = \\|V\\|$$\n",
    "\n",
    "where the Euclidean norm yields $\\|V\\| = \\sqrt{V^T V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-moderator",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "This follows from the fact that $$(SV)^T SV = (V^T S^T)SV = V^T (S^T S) V = V^T V$$\n",
    "\n",
    "So one may conclude that $(SV)^T SV = V^T V \\Longrightarrow \\|SV \\| = \\|V \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-bahrain",
   "metadata": {},
   "source": [
    "4) Consider a vector $V \\in \\mathbb{R}^N$, and define the matrix \n",
    "\n",
    "$$ S(V) = Id - 2 \\frac{V V^T}{\\|V\\|^2},$$ \n",
    "\n",
    "where $\\|V\\| = \\sqrt{V^T V}$ denotes the Euclidean norm of $V$. \n",
    "\n",
    "Prove that $S(V)$ is an orthogonal matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-polymer",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "\n",
    "Using that $\\| V\\| ^2 = \\sum v_j^2$ and $(VV^T)_{i,j} = v_i v_j$, where $(v_i)_i$ are the entries of the column $V$, then we may write:\n",
    "$$S(V)_{i,i} = 1 - 2\\frac{v_i ^2}{\\sum v_j^2} $$\n",
    "\n",
    "$$S(V)_{i,j} = -2\\frac{v_i v_j}{\\sum v_k^2}$$\n",
    "Whenever $i\\neq j$\n",
    "\n",
    "Hence, $S(V)$ is a symmetric matrix, so $S(V)^T = S(V)$. Proving that it is orthogonal is equivalent to proving that $S(V)^{-1} = S(V)$, or in other words, $S(V)^2 = Id$\n",
    "\n",
    "For this, we explicitly compute $S(V)^2$:\n",
    "$$(Id - 2\\frac{VV^T}{\\|V\\|^2})^2 = Id - 4\\frac{VV^T}{\\|V\\|^2} + 4\\frac{VV^T VV^T}{\\|V\\|^4}$$\n",
    "\n",
    "Since $(VV^T)^2 = V (V^T V) V^T = \\|V\\|^2 V V^T$, one can observe that $4\\frac{VV^T VV^T}{\\|V\\|^4} = 4\\frac{\\|V\\|^2 VV^T}{\\|V\\|^4} = 4\\frac{VV^T}{\\|V\\|^2}$, so the equation above simplifies to:\n",
    "\n",
    "$$S(V)S(V)^T = S(V)^2 = (Id - 2\\frac{VV^T}{\\|V\\|^2})^2 = Id$$\n",
    "\n",
    "So we can conclude that $S(V)$ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-guitar",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Construction of the algorithm\n",
    "\n",
    "As in the Gaussian elimination algorithm, we eliminate the subdiagonal coefficients of a matrix $A$. But, instead of using elementary matrices, we use orthogonal matrices $S(V)$ as defined in 4).  \n",
    "\n",
    "Let us rewrite $A = (C^1, C^2, \\dots, C^N)$ as a row vector of column vectors $C^i \\in \\mathbb{R}^N$. We multiply iteratively the matrix $A$, and therefore each of its column, by an orthogonal matrix $S(V^i)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-direction",
   "metadata": {},
   "source": [
    "5) Consider $C^1 \\in \\mathbb{R}^N$, we seek a vector $V^1 \\in \\mathbb{R}^N$ such that \n",
    "\n",
    "$$S(V^1) C^1 = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right). \\qquad{} (1) $$ \n",
    "\n",
    "What are the possible values of $\\alpha$? If there are several possibilities, choose one of them here for the rest of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d82b78",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "The general solutions are $\\alpha = \\pm \\|C\\| = \\pm \\sqrt{\\sum c_i ^2}$\n",
    "\n",
    "**Proof:**\n",
    "For this, we use that $S(V) C = (Id - 2\\frac{V V^T}{\\|V\\|^2})C = C - 2V \\frac{V^T C}{\\|V\\|^2} = C - tV$, where $t$ is a real number, namely $2\\frac{V^T C}{\\|V\\|^2} = 2\\frac{\\sum v_i c_i}{\\sum v_i ^2}$\n",
    "\n",
    "The case $t=0$ leads to the particular case $C = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)$, so $\\alpha = c_1$, but this should be considered only when $c_j = 0 \\forall j\\neq 1$, meaning that $\\|C\\| = \\sqrt{c_1^2}$ so the solution $\\alpha$ still satisfies $\\alpha \\in \\{-\\|C\\|^2,\\|C\\|^2\\}$\n",
    "\n",
    "For the general case $t\\neq 0$ we obtain $V = \\frac{1}{t} (C - \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right))$ so we have the equations:\n",
    "$$v_1 = \\frac{c_1 - \\alpha}{t}$$\n",
    "$$v_j = \\frac{c_j}{t} \\forall j\\neq 1$$\n",
    "\n",
    "Using these, we may compute the following:\n",
    "$$\\sum v_i c_i = \\frac{c_1-\\alpha}{t} c_1 + \\sum_{j\\neq1} \\frac{c_j^2}{t} = \\frac{\\sum c_i ^2}{t} - \\frac{\\alpha c_1}{t}$$\n",
    "$$\\sum v_i ^2 = \\frac{(c_1-\\alpha)^2}{t^2} + \\sum_{j\\neq 1} \\frac{c_j^2}{t^2} = \\frac{\\sum c_i^2}{t^2} + \\frac{\\alpha^2 - 2\\alpha c_1}{t^2} $$\n",
    "\n",
    "Hence, we go back to the equation $t = 2\\frac{\\sum v_i c_i}{\\sum v_i^2}$ and we can rewrite it as:\n",
    "$$t = 2t\\frac{\\sum c_i^2 - \\alpha c_1}{\\sum c_i^2 + \\alpha^2 - 2\\alpha c_1}$$\n",
    "\n",
    "Since $t$ is non-zero, we can simplify it and retrieve the equation:\n",
    "$$\\sum c_i^2 + \\alpha^2 - 2\\alpha c_1 = 2\\sum c_i ^2 - 2\\alpha c_1$$\n",
    "\n",
    "From which it follows that:\n",
    "$$\\alpha^2 = \\sum c_i^2 = \\|C\\|^2$$\n",
    "\n",
    "So we indeed we have $\\alpha \\in \\{ -\\|C\\|,\\|C\\|\\}$\n",
    "\n",
    "For better clarity, we may choose to work with $\\alpha = \\|C\\|$ for the remainder of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1faac9f",
   "metadata": {},
   "source": [
    "6) We seek $V^1\\in\\mathbb{R}^N$ satisfying (1) and such that $\\|V^1\\| = 1$. Let us decompose $V^1 = \\left( \\begin{array}{c} a \\\\ W \\end{array} \\right)$ and $C^1 = \\left( \\begin{array}{c} b \\\\ X \\end{array} \\right)$.\n",
    "\n",
    "a) Prove that $W = kX$ is proportional to $X$. \n",
    "\n",
    "b) Write $a$ as a function of $k$ and $X$.\n",
    "\n",
    "c) Write an equation satisfied by $k$ depending only on $X$ and $b$. \n",
    "\n",
    "d) Solve this equation and compute $k$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4734e7a",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) Recall from above that the equation (1) implies that $v_j = \\frac{1}{t} c_j$ $ \\forall j\\neq 1$, where $t$ is defined as before. Hence we have $W = kX$ with $k = \\frac{1}{t}$\n",
    "\n",
    "b) To write $a$ in terms of $k$ and $X$ we may use that $\\|V^1\\|^2 = a^2 + \\|W\\|^2 = a^2 + k^2 \\|X\\|^2$, so we have $a^2 = 1 - k^2 \\|X\\|^2$, and here we get that $a$ is of the form $\\pm \\sqrt{1-k^2\\|X\\|^2}$\n",
    "\n",
    "c) From the previous exercise we have that $v_1 = a = \\frac{1}{t} (c_1 - \\alpha) = k (b - \\|C\\|)$. With this we can compute:\n",
    "$$\\|V^1\\|^2 = a^2 + \\|W\\|^2 = k^2 (b-\\|C\\|)^2 + k^2 \\|X\\|^2 = k^2 (b^2 + \\|C\\|^2 - 2b\\|C\\| + \\|X\\|^2)$$\n",
    "\n",
    "Since $\\|C\\| = \\sqrt{b^2 + \\|X\\|^2}$, we can now rewrite the above equations just in terms of $k$,$b$ and $X$:\n",
    "$$1 = k^2(2b^2 + 2\\|X\\|^2 - 2b\\sqrt{b^2 + \\|X\\|^2})$$\n",
    "\n",
    "So it follows that:\n",
    "$$k^2 = \\frac{1}{2 (b^2 + \\|X\\|^2 - b\\sqrt{b^2+\\|X\\|^2})}$$\n",
    "\n",
    "d) Note that $b < \\|C\\|$ when $X \\neq 0$, so $a$ will have the opposite sign of $k$. Both roots lead to a unique solution in $V^1$, so both solutions lead to valid solutions:\n",
    "\n",
    "$$k = \\pm \\sqrt{\\frac{1}{2(b^2 + \\|X\\|^2 - b\\sqrt{b^2+\\|X\\|^2})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196acc27",
   "metadata": {},
   "source": [
    "7) a) What vector operations do you need to perform to compute $S(V^1) C^2$ for some other vector $C^2 \\in\\mathbb{R}^N$? \n",
    "\n",
    "b) How many scalar operations are therefore necessary to compute this product?\n",
    "\n",
    "c) How many scalar operations are therefore necessary to compute the product $S(V^1) A$, without performing the trivial operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0fc4c",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) Using that $S(V)C^2 = C^2 - tV$, where $t = 2\\frac{\\sum v_i c_i}{\\sum v_i^2} = 2\\frac{V^T C^2}{V^T V}$, the only vector operations one needs are the computation of $V^T C^2$,$V^T V$ and the linear operation $C^2 - tV$, so $3$ in total.\n",
    "\n",
    "b) When multiplying a $n\\times1$ row vector with a $1\\times n$ column vector, we have $n^2$ multiplications and $n-1$ additions to gather all of them in the same place. Moreover, when we perform a linear operation between two known column vectors of size $n\\times 1$, we perform n scalar operations. We have to keep in mind that we also need $2$ operations to obtain the $t$ and $n$ more to compute $tV$. Hence, to compute just one operation of the form $S(V)C$ we need: $2(n^2 + n-1) + n + 2 + n = 2(n^2 + 2n)$ scalar operations.\n",
    "\n",
    "c) Since we must do the same method for each of A's columns, the total number is equal to $2(n^3 + 2n^2) = \\mathcal{O}(n^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284ec57",
   "metadata": {},
   "source": [
    "8) Now, we need to eliminate the subdiagonal term on the second column without modifying the first column of $A^1 := S(V^1)A$, i.e. we want \n",
    "\n",
    "$$S(V^2) A^1 = \\left( \\begin{array}{c} * \\\\ \\beta \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right).$$\n",
    "\n",
    "For this purpose, we use another matrix $S(V^2)$ satisfying \n",
    "\n",
    "$$ S(V^2)_{1,i} = \\delta_{1,i}. $$\n",
    "\n",
    "What does this condition imposes on the entries of the vector $V^2$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b0eb8",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "If we write $v_1,v_2,\\cdots,v_n$ the entries of $V^2$ and $S = \\|V^2\\|^2 = \\sum v_i^2$, then the first row condition translates to $\\begin{pmatrix} 1 - 2\\frac{v_1^2}{S} & -2\\frac{v_1 v_2}{S} & -2\\frac{v_1 v_3}{s} &\\cdots & -2\\frac{v_1 v_n}{S}\\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 &\\cdots & 0 \\end{pmatrix}$, so it is clear that the condition $v_1 = 0$ is necessary and sufficient for $V^2$ to satisfy  $S(V^2)_{1,i} = \\delta_{1,i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa670",
   "metadata": {},
   "source": [
    "9) Again, what is the value of $\\beta$ ? And propose a vector $V^2 \\in \\mathbb{R}^N$ satisfying $\\|V^2\\| = 1$ and (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971af3f",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "We may write $V^2 = \\left(\\begin{array}{c} 0\\\\W \\end{array}  \\right)$ and let $C = \\left(\\begin{array}{c} c_1\\\\X \\end{array}  \\right)$ be the second column of $A^1$, then it is easy to see that $S(V^2) = \\begin{pmatrix} 1 &| & 0_{1\\times (n-1)}\\\\ \\hline 0_{(n-1) \\times 1} & | & S(W)  \\end{pmatrix}$, so it follows that:\n",
    "$$S(V^2)C = \\left(\\begin{array}{c} c_1 \\\\ S(W)X \\end{array} \\right)$$\n",
    "\n",
    "This leads us to the problem solved before: \"Find $W$ such that $S(W)X = \\left( \\begin{array}{c} \\beta \\\\ 0\\\\0\\\\ \\vdots\\\\0 \\end{array}  \\right)$\". \n",
    "\n",
    "Using the same arguments as before, we get $\\beta = \\pm \\|X\\|$, and the solutions where $\\|V^2\\|^2 = 1$ are of the form \n",
    "$$V^2 = \\left( \\begin{array}{c} 0 \\\\ k(c_1-\\|X\\|) \\\\ kX \\end{array} \\right)$$\n",
    ", where $k$ satisfies $k^2 = \\frac{1}{2 (c_1^2 + \\|X\\|^2 - c_1\\sqrt{c_1^2+\\|X\\|^2})}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d3fc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In practice, this technique is applied iteratively to all columns until writing a product of the form \n",
    "\n",
    "$$\\left(\\prod\\limits_i S(V^i) \\right) A = U $$\n",
    "\n",
    "which provides the decomposition $A = SU$ where $S = \\left(\\prod\\limits_i S(V^i) \\right)^T$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
