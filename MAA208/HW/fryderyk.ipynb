{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-painting",
   "metadata": {},
   "source": [
    "# Homework:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bf023e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NAME: Levente Ludanyi**\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:** \n",
    "- The clarity (clean writing) and the organization of the work (numbering the questions appropriately) are taken into account during the correction. \n",
    "- Before uploading your notebook, **Kernel/restart and clear output**. And **verify that your code is running cell after cell.**\n",
    "- This homework is **optional**. It counts at most as 4 **bonus** points on the final grade (over 20) of the course (capped at 20). It must be uploaded back on moodle **BEFORE** the 12th of March at 8PM (Paris time) to be counted, no delay will be accepted. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d9dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6f65d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64429d69",
   "metadata": {},
   "source": [
    "Consider the matrices \n",
    "\n",
    "$$A = \\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 2 & 3 & 0 \\\\ 4 & 5 & 6\\end{array}\\right), \\qquad \n",
    "  B = \\left(\\begin{array}{ccc} 1 & 2 & 1 \\\\ 2 & 5 & 4 \\\\ 1 & 4 & 6\\end{array}\\right), $$\n",
    "and the vector $b = \\left(\\begin{array}{c} 1 \\\\ 1 \\\\ 1\\end{array}\\right)$.\n",
    "\n",
    "1) Write down the steps of the forward substitution algorithm to solve the problem $AV = b$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab66c108",
   "metadata": {},
   "source": [
    "**Answer**: 1) \n",
    "\n",
    "In the forward substitution algorithm, we solve the system $Lx = b$ by working from the top. $L$ is a lower triangular matrix.\n",
    "\n",
    "$Lx = b$ can be written as system of equations:\n",
    "$$\\begin{cases}\n",
    "l_{11}x_1 = b_1 \\\\\n",
    "l_{21}x_1 + l_{22}x_2 = b_2 \\\\\n",
    "l_{31}x_1 + l_{32}x_2 + l_{33}x_3 = b_3 \\\\\n",
    "\\dots\n",
    "\\end{cases}$$\n",
    "\n",
    "as said before, we work from the top, so we can solve the first equation for $x_1$:\n",
    "$$x_1 = \\frac{b_1}{l_{11}}$$\n",
    "\n",
    "then we can solve the second equation for $x_2$:\n",
    "$$x_2 = \\frac{b_2 - l_{21}x_1}{l_{22}}$$\n",
    "\n",
    "and so on...\n",
    "\n",
    "Let's solve the system $Lx = b$ for $A$ and $b$ by hand:\n",
    "we have\n",
    "$$\\begin{cases}\n",
    "l_{11}x_1 =  \\\\\n",
    "l_{21}x_1 + l_{22}x_2 = b_2 \\\\\n",
    "l_{31}x_1 + l_{32}x_2 + l_{33}x_3 = b_3 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "which is\n",
    "$$\\begin{cases}\n",
    "1x_1 = 1 \\\\\n",
    "2x_1 + 3x_2 = 1 \\\\\n",
    "4x_1 + 5x_2 + 6x_3 = 1 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Following the algorithm, we can solve the first equation for $x_1$:\n",
    "$$x_1 = \\frac{1}{1} = 1$$\n",
    "\n",
    "then we can solve the second equation for $x_2$:\n",
    "$$x_2 = \\frac{1 - 2x_1}{3} = \\frac{1 - 2}{3} = \\frac{-1}{3}$$\n",
    "\n",
    "and finally we can solve the third equation for $x_3$:\n",
    "$$x_3 = \\frac{1 - 4x_1 - 5x_2}{6} = \\frac{1 - 4 - 5\\cdot\\frac{-1}{3}}{6} = \\frac{1 - 4 + \\frac{5}{3}}{6} = \\frac{-4}{18} = \\frac{-2}{9}$$\n",
    "\n",
    "Let's check the solution:\n",
    "$$\\begin{cases}\n",
    "1\\cdot 1= 1 \\\\\n",
    "2\\cdot 1 + 3\\cdot\\frac{-1}{3} = 1 \\\\\n",
    "4\\cdot 1 + 5\\cdot\\frac{-1}{3} + 6\\cdot\\frac{-2}{9} = 1 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76c4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.33333333 -0.22222222]\n"
     ]
    }
   ],
   "source": [
    "#Test our solution\n",
    "A = np.array([[1, 0, 0], [2, 3, 0], [4, 5, 6]])\n",
    "b = np.array([1, 1, 1])\n",
    "\n",
    "def forward_substitution(L, b):\n",
    "    \"\"\"\n",
    "    Compute the solution of a lower triangular system\n",
    "    ----------   \n",
    "    parameters:\n",
    "    L : lower triangular matrix (numpy array of size N,N)\n",
    "    b : matrix (numpy array of size N)\n",
    "    \n",
    "    returns:\n",
    "    V : solution of the linear problem (numpy array of size N)\n",
    "    \"\"\"\n",
    "    N = len(b)\n",
    "    V = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if(L[i, i] != 0):\n",
    "            V[i] = (b[i] - sum(L[i, j]*V[j] for j in range(i))) / L[i, i]\n",
    "        else:\n",
    "            return \"Input matrix is not invertible\"\n",
    "    return V\n",
    "\n",
    "print(forward_substitution(A, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af57e9b",
   "metadata": {},
   "source": [
    "2) Write down the steps of the Cholesky algorithm to decompose the matrix $B = L L^T$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea15da5",
   "metadata": {},
   "source": [
    "**Answer**: 2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc42034",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-producer",
   "metadata": {},
   "source": [
    "**Definition:** A matrix $S$ is orthogonal if it is invertible and its inverse satisfies $S^{-1} = S^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-voluntary",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to construct an algorithm providing a decomposition of a matrix $A = S U$ into the product of an orthogonal matrix $S$ and an upper triangular one $U$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-dryer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preliminary computations on orthogonal matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "greater-booth",
   "metadata": {},
   "source": [
    "1) a) Consider an orthogonal matrix $S$. What values can $det(S)$ have?\n",
    "\n",
    "\n",
    "\n",
    "b) Are all matrices satisfying this property orthogonal? Prove it or give a counterexample. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "brown-chemical",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a)\n",
    "We have $S^{-1} = S^T$, so $S^T S^{-1} = S^{-1} S^T = I$, hence $det(S^T S^{-1}) = det(S^T) det(S^{-1}) = det(I) = 1$. Using the fact $S^{-1} = S^T$ and $det(S) = det(S^T) = 1$ we have $(det(S))^2 = 1$. Therefore $$det(S) = \\pm 1$$\n",
    "\n",
    "b) \n",
    "No, not all matrices satisfying this property are orthogonal. For example, the matrix $S = \\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)$ satisfies the property, but it is not orthogonal. As\n",
    "$$S^{-1} = \\left(\\begin{array}{cc} 1 & -1 \\\\ 0 & 1\\end{array}\\right)$$ \n",
    "and\n",
    "$$S^T = \\left(\\begin{array}{cc} 1 & 0 \\\\ 1 & 1\\end{array}\\right)$$\n",
    "we have\n",
    "$$S^{-1} S^T = \\left(\\begin{array}{cc} 1 & -1 \\\\ 0 & 1\\end{array}\\right) \\left(\\begin{array}{cc} 1 & 0 \\\\ 1 & 1\\end{array}\\right) = \\left(\\begin{array}{cc} 0 & -1 \\\\ 1 & 1\\end{array}\\right)$$ \n",
    "which is not the 2-by-2 identity matrix so we found a counterexample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-vertical",
   "metadata": {},
   "source": [
    "2) Prove that the product of orthogonal matrices remains an orthogonal matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "controversial-accommodation",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "For the proof we are going to use two identities: $(AB)^T = B^T A^T$ and $(AB)^{-1} = B^{-1} A^{-1}$.\n",
    "\n",
    "Let us these statements.\n",
    "\n",
    "Let $A$ be a matrix of size $m \\times n$ and $B$ a matrix of size $n \\times p$, then $(AB)_{ji}^T = \\sum_{k=1}^{n} A_{ik}B_{kj}$ for j = 1, ..., p and i = 1, ..., m. Furthermore, $(B^T A^T)_{ji} = \\sum_{k=1}^{n} B_{kj}A_{ik}$  Notice that $A_{ik}B_{kj} = B_{kj}A_{ik}$ so we have $(AB)_{ji}^T = (B^T A^T)_{ji}$ for all j in [1,p] and i in [1,m]. Hence $(AB)^T = B^T A^T$.\n",
    "\n",
    "Now let us prove the second identity. Let $A$ and $B$ be $n \\times n$ matrices. By definition we have $AA^{-1} = I_n$ and $BB^{-1} = I_n$, so $(AB)(B^{-1}A^{-1}) =(A(BB^{-1}))A^{-1}$ by the associativity of matrix multiplication. $A(BB^{-1}))A^{-1} = (AI_n)A^{-1} = AA^{-1} = I_n$. Now, we have to check for multiplication from the other direction. We have $(B^{-1}A^{-1}) (AB) = (B^{-1}(A^{-1}A))B = (B^{-1}I_n)B = B^{-1}B = I_n$. So if we multiply $(AB)$ by $B^{-1}A^{-1}$ (from both directions), we got $I_n$, hence, $(AB)^{-1} = B^{-1} A^{-1}$.\n",
    "\n",
    "Take two orthogonal matrices $S_1$ and $S_2$. We have $S_1^{-1} = S_1^T$ and $S_2^{-1} = S_2^T$. Take their product $S = S_1 S_2$. We have $S^{-1} = S_2^{-1} S_1^{-1} = S_2^T S_1^T = (S_1 S_2)^T = S^T$. So $S$ is orthogonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-spyware",
   "metadata": {},
   "source": [
    "3) Prove that the Euclidean norm of a vector is preserved when multiplying it by an orthogonal matrix $S$, i.e. \n",
    "\n",
    "$$\\|S V\\| = \\|V\\|$$\n",
    "\n",
    "where the Euclidean norm yields $\\|V\\| = \\sqrt{V^T V}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sound-moderator",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "We have $\\|S V\\| = \\sqrt{(S V)^T (S V)} = \\sqrt{V^T S^T S V} = \\sqrt{V^T S^{-1} S V} = \\sqrt{V^T I V} = \\sqrt{V^T V} = \\|V\\|$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "rolled-bahrain",
   "metadata": {},
   "source": [
    "4) Consider a vector $V \\in \\mathbb{R}^N$, and define the matrix \n",
    "\n",
    "$$ S(V) = Id - 2 \\frac{V V^T}{\\|V\\|^2},$$ \n",
    "\n",
    "where $\\|V\\| = \\sqrt{V^T V}$ denotes the Euclidean norm of $V$. \n",
    "\n",
    "Prove that $S(V)$ is an orthogonal matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "broken-polymer",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "\n",
    "First compute $S(V)^T$. It is $(Id - 2 \\frac{V V^T}{\\|V\\|^2})^T = Id^T - (V^T V)^T \\frac{2}{\\|V\\|^2} = Id - V^T (V^T)^T \\frac{2}{\\|V\\|^2} = Id - V^T V \\frac{2}{\\|V\\|^2} = Id - V V^T \\frac{2}{\\|V\\|^2} = Id - 2 \\frac{V V^T}{V^T V} = S(V)$\n",
    "\n",
    "Hence $S(V)S(V)^T = S(V)S(V) = (Id - 2 \\frac{V V^T}{\\|V\\|^2})(Id - 2 \\frac{V V^T}{\\|V\\|^2}) = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{V(V^TV) V^T}{(\\|V\\|^2)^2} = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{V(V^TV) V^T}{(V^T V)^2} = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{VV^T}{V^T V} = Id$. We have the same result if we multiply $S(V)$ by $S(V)^T$ from the other direction as $S(V)^TS(V) = S(V)S(V) = S(V)S(V)^T $. Therefore we see $S(V)^T = S(V)^{-1}$. We can conclude $S(V)$ is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-guitar",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Construction of the algorithm\n",
    "\n",
    "As in the Gaussian elimination algorithm, we eliminate the subdiagonal coefficients of a matrix $A$. But, instead of using elementary matrices, we use orthogonal matrices $S(V)$ as defined in 4).  \n",
    "\n",
    "Let us rewrite $A = (C^1, C^2, \\dots, C^N)$ as a row vector of column vectors $C^i \\in \\mathbb{R}^N$. We multiply iteratively the matrix $A$, and therefore each of its column, by an orthogonal matrix $S(V^i)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-direction",
   "metadata": {},
   "source": [
    "5) Consider $C^1 \\in \\mathbb{R}^N$, we seek a vector $V^1 \\in \\mathbb{R}^N$ such that \n",
    "\n",
    "$$S(V^1) C^1 = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right). \\qquad{} (1) $$ \n",
    "\n",
    "What are the possible values of $\\alpha$? If there are several possibilities, choose one of them here for the rest of the algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58d82b78",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Recall $\\|S V\\| = \\|V\\|$, so  $\\|S(V^1) C^1\\| = \\|C^1\\|$ and we have $S(V^1) C^1 = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)$, so $ \\| S(V^1) C^1  \\| =  \\| \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)  \\|$, hence we have $\\alpha =  \\| C^1 \\|$. However, notice we can have $\\alpha =  - \\| C^1 \\|$ as well. For the rest of the algorithm we will use the one with plus sign. Altough let's think about which one is the optimal to use in general. Notice what we are doing with the transformation is a projection onto $e_1$ in either positive or negative directions. If $x$ is close to $\\|x\\| e_1$ we can loose precision so we always choose the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1faac9f",
   "metadata": {},
   "source": [
    "6) We seek $V^1\\in\\mathbb{R}^N$ satisfying (1) and such that $\\|V^1\\| = 1$. Let us decompose $V^1 = \\left( \\begin{array}{c} a \\\\ W \\end{array} \\right)$ and $C^1 = \\left( \\begin{array}{c} b \\\\ X \\end{array} \\right)$.\n",
    "\n",
    "a) Prove that $W = kX$ is proportional to $X$. \n",
    "\n",
    "b) Write $a$ as a function of $k$ and $X$.\n",
    "\n",
    "c) Write an equation satisfied by $k$ depending only on $X$ and $b$. \n",
    "\n",
    "d) Solve this equation and compute $k$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4734e7a",
   "metadata": {},
   "source": [
    "**Answer:** a)\n",
    "\n",
    "Recall\n",
    "$S(V^1) = Id - 2 \\frac{V^1 (V^1)^T}{\\|V^1\\|^2}$, we have $\\|V^1\\| = 1$, $V^1 = \\left( \\begin{array}{c} a \\\\ W \\end{array} \\right)$ and $C^1 = \\left( \\begin{array}{c} b \\\\ X \\end{array} \\right)$, so $S(V^1) = Id - 2 (\\left( \\begin{array}{c} a \\\\ W \\end{array} \\right) \\left( \\begin{array}{c} a \\\\ W \\end{array} \\right)^T) = \\left(\\begin{array}{cc} 1 - 2a^2 & -2aW \\\\ -2aW & 1 - 2W^2\\end{array}\\right)$.\n",
    "As $V^1$ satisfies (1) we have $S(V^1) C^1 = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)$, hence $\\left(\\begin{array}{cc} (1 - 2a^2)b & -2aWX \\\\ -2aWb & (1 - 2W^2)X \\end{array}\\right) = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)$.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "(1 - 2a^2)b -2aWX = \\alpha \\\\\n",
    "-2aWb + (1 - 2W^2)X = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{(1 - 2a^2)b -\\alpha}{2a} = WX \\\\\n",
    "-2aWb + X =  2W \\cdot WX  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{(1 - 2a^2)b -\\alpha}{2a} = WX \\\\\n",
    "-2aWb + X =  2W \\cdot \\frac{(1 - 2a^2)b -\\alpha}{2a}  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{(1 - 2a^2)b -\\alpha}{2a} = WX \\\\\n",
    " (2 \\cdot \\frac{(1 - 2a^2)b -\\alpha}{2a} + 2ab ) W = X \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "$a \\neq 0$\n",
    "\n",
    "So $(2 \\cdot \\frac{(1 - 2a^2)b -\\alpha}{2a} + 2ab ) W = \\frac{b -\\alpha}{a}  W = X$ meanning $W = \\frac{a}{b -\\alpha}X$, so $W = kX$ is proportional to $X$ ($b - \\alpha \\neq 0$).\n",
    "\n",
    "\n",
    "\n",
    "b) \n",
    "As we saw in the previous part, $k = \\frac{a}{b -\\alpha}$ so $a = k(b -\\alpha)$, also using first equation $$\\frac{(1 - 2a^2)b -\\alpha}{2a} = kX \\cdot X$$ \n",
    "$$\n",
    "\\frac{1 - 2a^2}{2k} = kX \\cdot X\n",
    "$$\n",
    "So we have $a = \\frac{1 -2k^2 X^2}{2}$\n",
    "\n",
    "c) \n",
    "\n",
    "Combining the two expressions for a above we have $a = \\frac{1 -2k^2 X^2}{2} = k(b -\\alpha)$, so $a = k(b -\\alpha) = \\frac{1 -2k^2 X^2}{2}$, so $k = \\frac{1 -2k^2 X^2}{2(b -\\alpha)}$ we can write it as $2X^2 k^2 + 2(b - \\alpha)k - 1 = 0$\n",
    "\n",
    "d) \n",
    "$2X^2 k^2 + 2(b - \\alpha)k - 1 = 0$ is a quadratic equation in $k$ so the solutions are $k = \\frac{-2(b - \\alpha) \\pm \\sqrt{4(b - \\alpha)^2 - 4(2X^2)}}{4X^2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196acc27",
   "metadata": {},
   "source": [
    "7) a) What vector operations do you need to perform to compute $S(V^1) C^2$ for some other vector $C^2 \\in\\mathbb{R}^N$? \n",
    "\n",
    "b) How many scalar operations are therefore necessary to compute this product?\n",
    "\n",
    "c) How many scalar operations are therefore necessary to compute the product $S(V^1) A$, without performing the trivial operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0fc4c",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) \n",
    "\n",
    "b) \n",
    "\n",
    "c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284ec57",
   "metadata": {},
   "source": [
    "8) Now, we need to eliminate the subdiagonal term on the second column without modifying the first column of $A^1 := S(V^1)A$, i.e. we want \n",
    "\n",
    "$$S(V^2) A^1 = \\left( \\begin{array}{c} * \\\\ \\beta \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right).$$\n",
    "\n",
    "For this purpose, we use another matrix $S(V^2)$ satisfying \n",
    "\n",
    "$$ S(V^2)_{1,i} = \\delta_{1,i}. $$\n",
    "\n",
    "What does this condition imposes on the entries of the vector $V^2$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b0eb8",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa670",
   "metadata": {},
   "source": [
    "9) Again, what is the value of $\\beta$ ? And propose a vector $V^2 \\in \\mathbb{R}^N$ satisfying $\\|V^2\\| = 1$ and (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971af3f",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d3fc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In practice, this technique is applied iteratively to all columns until writing a product of the form \n",
    "\n",
    "$$\\left(\\prod\\limits_i S(V^i) \\right) A = U $$\n",
    "\n",
    "which provides the decomposition $A = SU$ where $S = \\left(\\prod\\limits_i S(V^i) \\right)^T$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ae0af1e38d07d4d689efede6e6412bbd62d96723f2ce4333bca9a14d03093764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
