{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-painting",
   "metadata": {},
   "source": [
    "# Homework:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bf023e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NAME:** Erik Dziekonski Bautista\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:** \n",
    "- The clarity (clean writing) and the organization of the work (numbering the questions appropriately) are taken into account during the correction. \n",
    "- Before uploading your notebook, **Kernel/restart and clear output**. And **verify that your code is running cell after cell.**\n",
    "- This homework is **optional**. It counts at most as 4 **bonus** points on the final grade (over 20) of the course (capped at 20). It must be uploaded back on moodle **BEFORE** the 12th of March at 8PM (Paris time) to be counted, no delay will be accepted. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6f65d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64429d69",
   "metadata": {},
   "source": [
    "Consider the matrices \n",
    "\n",
    "$$A = \\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 2 & 3 & 0 \\\\ 4 & 5 & 6\\end{array}\\right), \\qquad \n",
    "  B = \\left(\\begin{array}{ccc} 1 & 2 & 1 \\\\ 2 & 5 & 4 \\\\ 1 & 4 & 6\\end{array}\\right), $$\n",
    "and the vector $b = \\left(\\begin{array}{c} 1 \\\\ 1 \\\\ 1\\end{array}\\right)$.\n",
    "\n",
    "1) Write down the steps of the forward substitution algorithm to solve the problem $AV = b$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab66c108",
   "metadata": {},
   "source": [
    "**Answer**: 1) \n",
    "- Iterations: For $i$ fron $1$ to $N$ do\n",
    "$$V_i = \\frac{b_i - \\sum\\limits_{j=1}^{i-1}L_{i,j}V_j}{L_{i,i}}$$\n",
    "\n",
    "We write $V = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix}$.\n",
    "\n",
    "The steps of the forward substitution algorithm for a 3x3 matrix are as follows:\n",
    "\n",
    "1. $v_1 = b_1 / a_{11}$\n",
    "\n",
    "2. Let $v_2 = \\frac{b_2 - a_{21} v_1}{a_{22}}$\n",
    "\n",
    "3. Let $v_3 = \\frac{b_3 - a_{31} v_1 - a_{32} v_2}{a_{33}}$\n",
    "\n",
    "4. We have found $V$.\n",
    "\n",
    "Using these steps we can solve the problem $AV = b$ for $V$ as follows:\n",
    "\n",
    "1. $v_1 = 1 / 1 = 1$\n",
    "\n",
    "2. $v_2 = \\frac{1 - 2 \\cdot 1}{3} = -\\frac{1}{3}$\n",
    "\n",
    "3. $v_3 = \\frac{1 - 4 \\cdot 1 - 5 \\cdot \\frac{-1}{3}}{6} = -\\frac{2}{9}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af57e9b",
   "metadata": {},
   "source": [
    "2) Write down the steps of the Cholesky algorithm to decompose the matrix $B = L L^T$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea15da5",
   "metadata": {},
   "source": [
    "**Answer**: 2) \n",
    "\n",
    "\n",
    "The steps of the Cholesky algorithm, which works for positive definite matrices is as follows:\n",
    "\n",
    "Iterate over $i$ from $1$ to $N$, where $N$ is the number of rows of $B$\n",
    "\n",
    "- Compute\n",
    "\n",
    "$$L_{i,i}=\\sqrt{B_{i,i}-\\sum_{k=1}^{i-1}L_{i,k}^2}$$\n",
    "\n",
    "- For $j$ from $i+1$ to $N$, compute\n",
    "\n",
    "$$L_{j,i}=\\frac{B_{j,i}-\\sum_{k=1}^{i-1}L_{ik}L_{j,k}}{L_{i,i}}$$\n",
    "\n",
    "Our matrix $B$ is positive definite, so we can use the Cholesky algorithm to decompose it as follows:\n",
    "\n",
    "1.\n",
    "\n",
    "$L_{1,1}=\\sqrt{B_{1,1}-0}=1$\n",
    "\n",
    "$L_{2,1}=\\frac{B_{2,1}-0}{L_{1,1}}=2/1=2$\n",
    "\n",
    "$L_{3,1}=\\frac{B_{3,1}-0}{L_{1,1}}=1/1=1$\n",
    "\n",
    "2.\n",
    "\n",
    "$L_{2,2}=\\sqrt{B_{2,2}-L_{2,1}^2}=\\sqrt{5-4}=1$\n",
    "\n",
    "$L_{3,2}=\\frac{B_{3,2}-L_{2,1}L_{3,1}}{L_{2,2}}=\\frac{4-2*1}{1}=2$\n",
    "\n",
    "3.\n",
    "\n",
    "$L_{33}=\\sqrt{B_{3,3}-(L_{31}^2+L_{3,2}^2)}=\\sqrt{6-(1^2+2^2)}=1$\n",
    "\n",
    "4. We find that\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can verify our answer by checking that\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{pmatrix}\n",
    "\\times\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 5 & 4 \\\\\n",
    "1 & 4 & 6\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc42034",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-producer",
   "metadata": {},
   "source": [
    "**Definition:** A matrix $S$ is orthogonal if it is invertible and its inverse satisfies $S^{-1} = S^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-voluntary",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to construct an algorithm providing a decomposition of a matrix $A = S U$ into the product of an orthogonal matrix $S$ and an upper triangular one $U$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-dryer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preliminary computations on orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-booth",
   "metadata": {},
   "source": [
    "1) a) Consider an orthogonal matrix $S$. What values can $det(S)$ have?\n",
    "\n",
    "b) Are all matrices satisfying this property orthogonal? Prove it or give a counterexample. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "brown-chemical",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a)\n",
    "The determinant of an orthogonal matrix $S$ can only take the values $\\pm 1$. This is because the determinant of a product of matrices is the product of the determinants of the individual matrices, and $S^T S = I$ implies that $det(S^T S) = det(I) = 1$. But $det(S^T S) = det(S^T) det(S) = (det(S))^2$, so we have $(det(S))^2 = 1$, which means that $det(S) = \\pm 1$.\n",
    "\n",
    "Therefore, the determinant of an orthogonal matrix can only take the values $+1$ or $-1$.\n",
    "\n",
    "b)No, not all matrices satisfying this property ($\\det(S) = \\pm 1$) are orthogonal. For example, the matrix $S = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$ satisfies this property ($\\det(S) = 1$), but is not orthogonal. Since its inverse is $S^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\neq  \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} = S^T$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-vertical",
   "metadata": {},
   "source": [
    "2) Prove that the product of orthogonal matrices remains an orthogonal matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "controversial-accommodation",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Let $A$ and $B$ be two orthogonal matrices of the same size. We want to show that their product $AB$ is also an orthogonal matrix.\n",
    "\n",
    "To do so, we need to show that $(AB)^T(AB) = I$, where $I$ is the identity matrix of the appropriate size. We have\n",
    "\n",
    "$$(AB)^T(AB) = B^TA^TAB = B^TB,$$\n",
    "\n",
    "where we used the fact that $A^TA = I$ because $A$ is orthogonal. Similarly, we have $AA^T = I$ because $A$ is orthogonal, so $B^TB = I$ because $B$ is orthogonal. Therefore,\n",
    "\n",
    "$$(AB)^T(AB) = B^TB = I.$$\n",
    "\n",
    "This shows that $AB$ is an orthogonal matrix, as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-spyware",
   "metadata": {},
   "source": [
    "3) Prove that the Euclidean norm of a vector is preserved when multiplying it by an orthogonal matrix $S$, i.e. \n",
    "\n",
    "$$\\|S V\\| = \\|V\\|$$\n",
    "\n",
    "where the Euclidean norm yields $\\|V\\| = \\sqrt{V^T V}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sound-moderator",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Let $V$ be a vector and $S$ be an orthogonal matrix. We want to show that $\\|SV\\| = \\|V\\|$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\\begin{aligned}\\|SV\\|^2 &= (SV)^T(SV) \\\\\n",
    "&= V^TS^TSV \\\\\n",
    "&= V^TV \\qquad (\\text{since } S^TS = I \\text{ by definition of orthogonal matrix}) \\\\\n",
    "&= \\|V\\|^2\n",
    "\\end{aligned}$$\n",
    "Taking the square root of both sides yields $\\|SV\\| = \\|V\\|$, as desired. Therefore, the Euclidean norm of a vector is preserved under multiplication by an orthogonal matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-bahrain",
   "metadata": {},
   "source": [
    "4) Consider a vector $V \\in \\mathbb{R}^N$, and define the matrix \n",
    "\n",
    "$$ S(V) = Id - 2 \\frac{V V^T}{\\|V\\|^2},$$ \n",
    "\n",
    "where $\\|V\\| = \\sqrt{V^T V}$ denotes the Euclidean norm of $V$. \n",
    "\n",
    "Prove that $S(V)$ is an orthogonal matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "broken-polymer",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "To prove that a $S(V)$ is orthogonal, we need to show that it satisfies the conditions for orthogonality:\n",
    "\n",
    "- $S(V)$ is invertible.\n",
    "- $S(V)^T = S(V)^{-1}\\\\$\n",
    "First, we show that $S(V)$ is invertible. Since $V$ is nonzero, the matrix $\\frac{VV^T}{|V|^2}$ has rank 1, and its nullspace is one-dimensional. Therefore, the eigenvalues of this matrix are 0 with multiplicity $N-1$ and $\\frac{|V|^2}{|V|^2} = 1$ with multiplicity 1. It follows that the eigenvalues of $I - \\frac{2VV^T}{|V|^2}$ are $1$ with multiplicity $N-1$ and $-1$ with multiplicity $1$. Since all the eigenvalues are nonzero, the matrix is invertible.\n",
    "\n",
    "Next, we show that $S(V)^T = S(V)^{-1}$. We have:\n",
    "$$\\begin{aligned}\n",
    "S(V)^T &= (I - 2\\frac{VV^T}{\\|V\\|^2})^T \\\\\n",
    "&= I^T - (2\\frac{VV^T}{\\|V\\|^2})^T \\\\\n",
    "&= I - 2\\frac{(V^T)^TV^T}{\\|V\\|^2} \\\\\n",
    "&= I - 2\\frac{V V^T}{\\|V\\|^2} \\\\\n",
    "&= S(V).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Hence $S(V)S(V)^T = S(V)S(V) = (Id - 2 \\frac{V V^T}{\\|V\\|^2})(Id - 2 \\frac{V V^T}{\\|V\\|^2}) = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{V(V^TV) V^T}{(\\|V\\|^2)^2} = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{V(V^TV) V^T}{(V^T V)^2} = Id - 4 \\frac{V V^T}{\\|V\\|^2} + 4 \\frac{VV^T}{V^T V} = Id$.\n",
    "Proving that $S(V)^T=S(V)^{-1}$\n",
    "\n",
    "Therefore, $S(V)$ satisfies both conditions for orthogonality, and hence is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-guitar",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Construction of the algorithm\n",
    "\n",
    "As in the Gaussian elimination algorithm, we eliminate the subdiagonal coefficients of a matrix $A$. But, instead of using elementary matrices, we use orthogonal matrices $S(V)$ as defined in 4).  \n",
    "\n",
    "Let us rewrite $A = (C^1, C^2, \\dots, C^N)$ as a row vector of column vectors $C^i \\in \\mathbb{R}^N$. We multiply iteratively the matrix $A$, and therefore each of its column, by an orthogonal matrix $S(V^i)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-direction",
   "metadata": {},
   "source": [
    "5) Consider $C^1 \\in \\mathbb{R}^N$, we seek a vector $V^1 \\in \\mathbb{R}^N$ such that \n",
    "\n",
    "$$S(V^1) C^1 = \\left( \\begin{array}{c} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right). \\qquad{} (1) $$ \n",
    "\n",
    "What are the possible values of $\\alpha$? If there are several possibilities, choose one of them here for the rest of the algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58d82b78",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Remember that $|S V| = |V|$. Therefore, we have $|S(V^1) C^1| = |C^1|$. Also, we know that $S(V^1) C^1 = \\begin{pmatrix} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} $. This means that $\\| S(V^1) C^1 \\| = \\| \\begin{pmatrix} \\alpha \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}  \\|$, and as a result, we get $\\alpha = \\| C^1 \\|$. However, it is important to note that we could also have $\\alpha = - \\| C^1 \\|$ instead. For the rest of the algorithm, we will use the former (positive) value.\n",
    "\n",
    "It's worth considering which value of $\\alpha$ is preferable in general. The transformation we are performing is a projection onto the first standard basis vector $e_1$, either in the positive or negative direction. If the vector $x$ we are projecting is close to $|x| e_1$, we can lose precision due to floating point errors. To avoid this, we always choose the opposite sign of $\\alpha$ when $x$ is close to $|x| e_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1faac9f",
   "metadata": {},
   "source": [
    "6) We seek $V^1\\in\\mathbb{R}^N$ satisfying (1) and such that $\\|V^1\\| = 1$. Let us decompose $V^1 = \\left( \\begin{array}{c} a \\\\ W \\end{array} \\right)$ and $C^1 = \\left( \\begin{array}{c} b \\\\ X \\end{array} \\right)$.\n",
    "\n",
    "a) Prove that $W = kX$ is proportional to $X$. \n",
    "\n",
    "b) Write $a$ as a function of $k$ and $X$.\n",
    "\n",
    "c) Write an equation satisfied by $k$ depending only on $X$ and $b$. \n",
    "\n",
    "d) Solve this equation and compute $k$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4734e7a",
   "metadata": {},
   "source": [
    "**Answer:** a)\n",
    "\n",
    "b) \n",
    "\n",
    "c) \n",
    "\n",
    "d) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196acc27",
   "metadata": {},
   "source": [
    "7) a) What vector operations do you need to perform to compute $S(V^1) C^2$ for some other vector $C^2 \\in\\mathbb{R}^N$? \n",
    "\n",
    "b) How many scalar operations are therefore necessary to compute this product?\n",
    "\n",
    "c) How many scalar operations are therefore necessary to compute the product $S(V^1) A$, without performing the trivial operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0fc4c",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) \n",
    "\n",
    "b) \n",
    "\n",
    "c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284ec57",
   "metadata": {},
   "source": [
    "8) Now, we need to eliminate the subdiagonal term on the second column without modifying the first column of $A^1 := S(V^1)A$, i.e. we want \n",
    "\n",
    "$$S(V^2) A^1 = \\left( \\begin{array}{c} * \\\\ \\beta \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right).$$\n",
    "\n",
    "For this purpose, we use another matrix $S(V^2)$ satisfying \n",
    "\n",
    "$$ S(V^2)_{1,i} = \\delta_{1,i}. $$\n",
    "\n",
    "What does this condition imposes on the entries of the vector $V^2$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b0eb8",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa670",
   "metadata": {},
   "source": [
    "9) Again, what is the value of $\\beta$ ? And propose a vector $V^2 \\in \\mathbb{R}^N$ satisfying $\\|V^2\\| = 1$ and (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971af3f",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d3fc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In practice, this technique is applied iteratively to all columns until writing a product of the form \n",
    "\n",
    "$$\\left(\\prod\\limits_i S(V^i) \\right) A = U $$\n",
    "\n",
    "which provides the decomposition $A = SU$ where $S = \\left(\\prod\\limits_i S(V^i) \\right)^T$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
